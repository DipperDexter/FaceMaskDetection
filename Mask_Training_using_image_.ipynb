{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "622ca68723dc0c0369323b13a55f73fcd4397be05396566d5b5bee70b0408944"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "# above code just loads the model we prepared to check the masks\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototxtPath=os.path.sep.join([r'/home/aayush/Desktop/Coding/Python/Face_detector','deploy.prototxt'])\n",
    "weightsPath=os.path.sep.join([r'/home/aayush/Desktop/Coding/Python/Face_detector','res10_300x300_ssd_iter_140000.caffemodel'])\n",
    "#we are not use harcasscade using caffemodel as it is much more better than harcasscade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/aayush/Desktop/Coding/Python/Face_detector/deploy.prototxt'"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "prototxtPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/aayush/Desktop/Coding/Python/Face_detector/res10_300x300_ssd_iter_140000.caffemodel'"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "weightsPath\n",
    "#above are two mainmiles which detect the face \n",
    "#it is Single short Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=cv2.dnn.readNet(prototxtPath,weightsPath)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading our model\n",
    "model=load_model(r'/home/aayush/Desktop/Coding/Python/mobilenet_v2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the image to be tested\n",
    "image=cv2.imread(r'/home/aayush/Desktop/Coding/Python/wm2.jpeg')\n",
    "#imread takes an image and convert into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[232, 223, 214],\n",
       "        [232, 223, 214],\n",
       "        [232, 223, 214],\n",
       "        ...,\n",
       "        [ 62,  89,  99],\n",
       "        [ 68,  93, 103],\n",
       "        [ 78, 103, 113]],\n",
       "\n",
       "       [[232, 223, 214],\n",
       "        [232, 223, 214],\n",
       "        [232, 223, 214],\n",
       "        ...,\n",
       "        [ 67,  96, 105],\n",
       "        [ 67,  94, 104],\n",
       "        [ 66,  93, 103]],\n",
       "\n",
       "       [[232, 223, 214],\n",
       "        [232, 223, 214],\n",
       "        [232, 223, 214],\n",
       "        ...,\n",
       "        [ 63,  93, 104],\n",
       "        [ 68,  96, 107],\n",
       "        [ 66,  94, 105]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[200, 189, 181],\n",
       "        [199, 188, 180],\n",
       "        [199, 188, 180],\n",
       "        ...,\n",
       "        [151, 148, 140],\n",
       "        [151, 148, 140],\n",
       "        [152, 149, 141]],\n",
       "\n",
       "       [[199, 188, 180],\n",
       "        [199, 188, 180],\n",
       "        [198, 187, 179],\n",
       "        ...,\n",
       "        [152, 149, 141],\n",
       "        [153, 150, 142],\n",
       "        [153, 150, 142]],\n",
       "\n",
       "       [[199, 188, 180],\n",
       "        [199, 188, 180],\n",
       "        [198, 187, 179],\n",
       "        ...,\n",
       "        [153, 150, 142],\n",
       "        [154, 151, 143],\n",
       "        [155, 152, 144]]], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(668, 1000, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "image.shape\n",
    "#below shows the size of image and 3 is RGB channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h,w)=image.shape[:2]\n",
    "#we need 1280 and 720 i.e height and weight of image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(668, 1000)"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "(h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing it \n",
    "#Resize the image\n",
    "#image,1.0:-1.0 is an scaling factor and we mentioned here that we are not scaling it\n",
    "#here 104.0,117.0,123.0 is the colour channel\n",
    "#blob takes into in BGR not RGB so this function by defaukts inputs in BGR\n",
    "blob=cv2.dnn.blobFromImage(image,1.0,(300, 300),(104.0,177.0,123.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[[128., 127., 128., ...,  37., -44., -37.],\n",
       "         [128., 127., 128., ...,  80.,   6., -43.],\n",
       "         [128., 127., 128., ...,  95.,  69., -25.],\n",
       "         ...,\n",
       "         [ 95.,  93.,  91., ...,  39.,  39.,  41.],\n",
       "         [ 95.,  92.,  91., ...,  47.,  47.,  47.],\n",
       "         [ 95.,  92.,  91., ...,  47.,  49.,  49.]],\n",
       "\n",
       "        [[ 46.,  45.,  44., ...,  -2., -83., -84.],\n",
       "         [ 46.,  45.,  44., ...,  36., -32., -85.],\n",
       "         [ 46.,  45.,  44., ...,  45.,  29., -63.],\n",
       "         ...,\n",
       "         [ 11.,   9.,   7., ..., -37., -36., -35.],\n",
       "         [ 11.,   8.,   7., ..., -31., -29., -29.],\n",
       "         [ 11.,   8.,   7., ..., -29., -27., -27.]],\n",
       "\n",
       "        [[ 91.,  90.,  90., ...,  67., -20., -20.],\n",
       "         [ 91.,  90.,  90., ..., 109.,  35., -20.],\n",
       "         [ 91.,  90.,  90., ..., 116.,  99.,   5.],\n",
       "         ...,\n",
       "         [ 57.,  55.,  53., ...,   8.,   9.,  11.],\n",
       "         [ 57.,  54.,  53., ...,  14.,  17.,  17.],\n",
       "         [ 57.,  54.,  53., ...,  17.,  19.,  19.]]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "blob\n",
    "#images getting Scaled down annd resized the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 3, 300, 300)"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "blob.shape\n",
    "#its in $D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.setInput(blob)\n",
    "detections=net.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[[0.        , 1.        , 0.97395396, ..., 0.28215754,\n",
       "          0.6688398 , 0.59488446],\n",
       "         [0.        , 1.        , 0.17017119, ..., 0.5486332 ,\n",
       "          0.4041294 , 0.6962155 ],\n",
       "         [0.        , 1.        , 0.12901571, ..., 0.512806  ,\n",
       "          0.4563606 , 0.6793694 ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "detections\n",
    "#these arrays are indicating the face region of the image\n",
    "#that is our face detector model detected all the faces now we shall check for the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 1, 200, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "detections.shape\n",
    "#this 200 is the shape portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now here we shall be detecting the face with mask or without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over the detections\n",
    "\n",
    "\n",
    "#detections[2] is the face portion\n",
    "for i in range(0,detections.shape[2]):\n",
    "\n",
    "    confidence=detections[0,0,i,2]\n",
    "    \n",
    "    #confidence is the accuracy level which checks weather the person is wearing mask       or not\n",
    "    \n",
    "    if confidence>0.5:\n",
    "        #we need the X,Y coordinates\n",
    "        #creating the box portion indication the mask and face portion\n",
    "        box=detections[0,0,i,3:7]*np.array([w,h,w,h])\n",
    "\n",
    "\n",
    "        (startX,startY,endX,endY)=box.astype('int')\n",
    "        \n",
    "        #ensure the bounding boxes fall within the dimensions of the frame\n",
    "        #that is the box indicating the mask or frame portion shouldn't go outside the frame section\n",
    "        (startX,startY)=(max(0,startX),max(0,startY))\n",
    "        (endX,endY)=(min(w-1,endX), min(h-1,endY))\n",
    "        \n",
    "        \n",
    "        #extract the face ROI, convert it from BGR to RGB channel, resize it to 224,224 and preprocess it\n",
    "        \n",
    "\n",
    "        face=image[startY:endY, startX:endX]\n",
    "        #blob took input in BGR so converting it to RGB\n",
    "        face=cv2.cvtColor(face,cv2.COLOR_BGR2RGB)\n",
    "        #we trained our model to detect mask for size 224,224\n",
    "        face=cv2.resize(face,(224,224))\n",
    "        face=img_to_array(face)\n",
    "        face=preprocess_input(face)\n",
    "        face=np.expand_dims(face,axis=0)\n",
    "        \n",
    "        (mask,withoutMask)=model.predict(face)[0]\n",
    "        \n",
    "        #determine the class label and color we will use to draw the bounding box and text\n",
    "        label='Mask' if mask>withoutMask else 'No Mask'\n",
    "        #green color if mask else in red color BGR\n",
    "        color=(0,255,0) if label=='Mask' else (0,0,255)\n",
    "        \n",
    "        #include the probability in the label\n",
    "        #till 2 places of decimal\n",
    "        label=\"{}: {:.2f}%\".format(label,max(mask,withoutMask)*100)\n",
    "        \n",
    "        #display the label and bounding boxes\n",
    "        cv2.putText(image,label,(startX,startY-10),cv2.FONT_HERSHEY_SIMPLEX,0.40,color,2)\n",
    "        cv2.rectangle(image,(startX,startY),(endX,endY),color,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows the ouput screen\n",
    "cv2.imshow(\"OutPut\",image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}